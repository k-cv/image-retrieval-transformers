{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concat.ipynb\n",
    "- 2つのモデルで特徴を抽出した後、2つの特徴ベクトルをconcatして1つの特徴ベクトルを作る\n",
    "- そのベクトルを用いて、距離学習を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "モデル1: ViT -> Transformer ->\n",
    "input: 画像\n",
    "flow: image[batch,3,H,W] -> ViT=features[batch, sequence_length, dimension] -> Transformer = [batch, 256, 384]\n",
    "\"\"\"\n",
    "class Model1(nn.Module): #SimpleViT\n",
    "    \"\"\"\n",
    "    x[int] = [batch_size=8, H=224, W=224]\n",
    "    out: [batch=8, dim=256]\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dim, img_size=224, patch_size=16, in_channels=3, embed_dim=384, num_patches=196):\n",
    "        super(Model1, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        # パッチ埋め込み: 画像をパッチに分割し、埋め込み次元に変換\n",
    "        self.patch_embed = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # ポジションエンコーディング: パッチの位置情報を保持\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=8, dim_feedforward=embed_dim * 4, activation=\"gelu\")\n",
    "        # Transformerブロック: シーケンスデータを処理\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "        # 正規化層\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.sequence_to_vector = SequenceToVector(input_dim=embed_dim, output_dim=256, sequence_length=num_patches)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 入力画像のサイズチェック\n",
    "        assert x.shape[2] == self.img_size and x.shape[3] == self.img_size, \\\n",
    "            f\"Input image size must be ({self.img_size}, {self.img_size})\"\n",
    "\n",
    "        # パッチ埋め込み (B, C, H, W) -> (B, E, num_patches^0.5, num_patches^0.5)\n",
    "        x = self.patch_embed(x)  # [batch_size, embed_dim, H/patch_size, W/patch_size]\n",
    "        B, E, H, W = x.shape\n",
    "\n",
    "        # フラット化してシーケンスデータに変換 (B, E, H*W) -> (B, H*W, E)\n",
    "        x = x.flatten(2).transpose(1, 2)  # [batch_size, num_patches, embed_dim]\n",
    "\n",
    "        # ポジションエンコーディングを加える\n",
    "        x = x + self.pos_embedding\n",
    "\n",
    "        # Transformerを通す (B, num_patches, embed_dim)\n",
    "        x = x.transpose(0, 1)  # [num_patches, batch_size, embed_dim]\n",
    "        x = self.transformer(x)  # [num_patches, batch_size, embed_dim]\n",
    "        x = x.transpose(0, 1)  # [batch_size, num_patches, embed_dim]\n",
    "\n",
    "        # 正規化\n",
    "        x = self.norm(x)  # [batch_size, num_patches, embed_dim]\n",
    "        x = self.sequence_to_vector(x)\n",
    "        return x\n",
    "\n",
    "\"\"\"Model2 幾何モデル\n",
    "\n",
    "Returns:\n",
    "    _type_: _description_\n",
    "\"\"\"\n",
    "class Model2(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model2, self).__init__()\n",
    "        self.image_to_vector = ImageToVector(input_channels=3, img_size=224, embed_dim=256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mid = self.image_to_vector(x)\n",
    "        return mid\n",
    "    \n",
    "class ImageToVector(nn.Module):\n",
    "    def __init__(self, input_channels=3, img_size=224, embed_dim=384):\n",
    "        super(ImageToVector, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.img_size = img_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # 画像を平坦化して埋め込み次元に変換する線形層\n",
    "        self.flatten = nn.Flatten()  # (B, C, H, W) -> (B, C * H * W)\n",
    "        self.fc = nn.Linear(input_channels * img_size * img_size, embed_dim)  # フラット化 -> 埋め込み\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 入力画像のサイズをチェック\n",
    "        assert x.shape[1:] == (self.input_channels, self.img_size, self.img_size), \\\n",
    "            f\"Input size must be [batch_size, {self.input_channels}, {self.img_size}, {self.img_size}]\"\n",
    "\n",
    "        # フラット化\n",
    "        x = self.flatten(x)  # (B, 3, 224, 224) -> (B, 3 * 224 * 224)\n",
    "\n",
    "        # 線形層でベクトルに変換\n",
    "        x = self.fc(x)  # (B, 3 * 224 * 224) -> (B, 384)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SequenceToVector(nn.Module):\n",
    "    \"\"\"\n",
    "    x = [batch, seq_length, input_dim]\n",
    "    out: [batch, output_dim=256]\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=384, output_dim=256, sequence_length=196):\n",
    "        super(SequenceToVector, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # 自己注意を用いた重要な情報の抽出\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=8)\n",
    "\n",
    "        # 線形層で最終的な次元に変換\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "        # 正規化層\n",
    "        self.norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        入力: x (batch, sequence_length, input_dim)\n",
    "        出力: (batch, output_dim)\n",
    "        \"\"\"\n",
    "        # 転置してMultiheadAttentionの入力形式に合わせる (batch, sequence, dim) -> (sequence, batch, dim)\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # 自己注意で重要な特徴を抽出 (sequence, batch, dim)\n",
    "        attention_output, _ = self.attention(x, x, x)\n",
    "\n",
    "        # 平均プーリングでシーケンス次元を圧縮 (sequence, batch, dim) -> (batch, dim)\n",
    "        pooled_output = attention_output.mean(dim=0)\n",
    "\n",
    "        # 線形層で次元を縮小\n",
    "        output = self.fc(pooled_output)\n",
    "\n",
    "        # 正規化\n",
    "        output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "# MLP: 5x次元を4x次元に圧縮\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=256, hidden_dim=512, output_dim=256):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),  # 256 → 512\n",
    "            nn.ReLU(),                        # 活性化関数\n",
    "            nn.Linear(hidden_dim, output_dim) # 512 → 256\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.model1 = Model1(input_dim)        # モデル1: [BS, 256]\n",
    "        self.model2 = Model2(input_dim)        # モデル2: [BS, ]\n",
    "        self.mlp = MLP()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 各モデルから特徴ベクトルを取得\n",
    "        feature1 = self.model1(x)  # [バッチサイズ, 2x]\n",
    "        feature2 = self.model2(x)  # [バッチサイズ, 3x]\n",
    "\n",
    "        from icecream import ic\n",
    "        ic(feature1.shape, feature2.shape)\n",
    "        # 特徴ベクトルを結合\n",
    "        combined_features = torch.cat((feature1, feature2), dim=1)  # [バッチサイズ, 5x]\n",
    "        # MLPに通して圧縮\n",
    "        final_features = self.mlp(combined_features)  # [バッチサイズ, 4x]\n",
    "        return final_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CosineSimilarity\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, optimizer, device, margin=0.5, num_epochs=3):\n",
    "    \"\"\"\n",
    "    モデルの複数エポック分の学習を行う関数\n",
    "\n",
    "    Args:\n",
    "        model: CombinedModel インスタンス\n",
    "        dataloader: DataLoader インスタンス\n",
    "        optimizer: Optimizer インスタンス\n",
    "        device: 使用するデバイス（例: 'cuda'）\n",
    "        margin: Margin Ranking Loss のマージン値\n",
    "        num_epochs: 学習するエポック数\n",
    "\n",
    "    Returns:\n",
    "        各エポックの平均損失値のリスト\n",
    "    \"\"\"\n",
    "    epoch_losses = []  # 各エポックの平均損失を記録\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # 学習モード\n",
    "        total_loss = 0\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "        for batch in tqdm(dataloader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "            images, labels = batch  # DataLoaderからデータを取得\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # 出力特徴ベクトルの取得\n",
    "            features = model(images)  # [batch_size, 256]\n",
    "\n",
    "            # 類似度の計算（上位半分と下位半分のペア）\n",
    "            batch_size = images.size(0)\n",
    "            similarity = CosineSimilarity(dim=1)(\n",
    "                features[:batch_size // 2],\n",
    "                features[batch_size // 2:]\n",
    "            )\n",
    "\n",
    "            # Contrastive Loss の計算\n",
    "            target = torch.zeros_like(similarity)  # すべてのペアが異なるものとして仮定\n",
    "            loss = F.margin_ranking_loss(\n",
    "                similarity,\n",
    "                target,\n",
    "                labels[:batch_size // 2].float(),\n",
    "                margin=margin\n",
    "            )\n",
    "\n",
    "            # 勾配の計算とパラメータの更新\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # ロスを蓄積\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # 平均損失値を計算して記録\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        epoch_losses.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return epoch_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/10 [00:00<?, ?it/s]ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "Training Epoch 1:  30%|███       | 3/10 [00:00<00:00, 23.73it/s]ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "Training Epoch 1:  60%|██████    | 6/10 [00:00<00:00, 24.17it/s]ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "Training Epoch 1:  90%|█████████ | 9/10 [00:00<00:00, 24.35it/s]ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "Training Epoch 1: 100%|██████████| 10/10 [00:00<00:00, 24.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] - Average Loss: 0.2750\n",
      "Epoch [2/3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:   0%|          | 0/10 [00:00<?, ?it/s]ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "Training Epoch 2:  30%|███       | 3/10 [00:00<00:00, 24.42it/s]ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "Training Epoch 2:  60%|██████    | 6/10 [00:00<00:00, 24.71it/s]ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "Training Epoch 2:  90%|█████████ | 9/10 [00:00<00:00, 24.37it/s]ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "Training Epoch 2: 100%|██████████| 10/10 [00:00<00:00, 24.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3] - Average Loss: 0.2125\n",
      "Epoch [3/3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:   0%|          | 0/10 [00:00<?, ?it/s]ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "Training Epoch 3:  30%|███       | 3/10 [00:00<00:00, 24.75it/s]ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "Training Epoch 3:  60%|██████    | 6/10 [00:00<00:00, 23.95it/s]ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "Training Epoch 3:  90%|█████████ | 9/10 [00:00<00:00, 22.74it/s]ic| feature1.shape: torch.Size([8, 256])\n",
      "    feature2.shape: torch.Size([8, 256])\n",
      "Training Epoch 3: 100%|██████████| 10/10 [00:00<00:00, 23.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3] - Average Loss: 0.2250\n",
      "Epoch Losses: [0.275, 0.2125, 0.225]\n"
     ]
    }
   ],
   "source": [
    "# ダミーデータの準備\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 8\n",
    "input_dim = 24  # 入力次元\n",
    "num_batches = 10\n",
    "num_samples = batch_size * num_batches\n",
    "\n",
    "# ランダムな画像とラベルデータを生成\n",
    "images = torch.randn(num_samples, 3, 224, 224)\n",
    "labels = torch.randint(0, 2, (num_samples,))  # 0: 異なるペア, 1: 同じペア\n",
    "\n",
    "# DataLoader\n",
    "dataset = TensorDataset(images, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# モデルとオプティマイザの準備\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "combined_model = CombinedModel(input_dim).to(device)\n",
    "optimizer = Adam(combined_model.parameters(), lr=1e-3)\n",
    "\n",
    "# 3エポック分の学習\n",
    "epoch_losses = train(combined_model, dataloader, optimizer, device, num_epochs=3)\n",
    "print(f\"Epoch Losses: {epoch_losses}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
